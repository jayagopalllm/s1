{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayagopalllm/s1/blob/main/BovineMetrics_Pilot_Model_v0_5_(Masked_Multi_View).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Project BovineMetrics - Pilot Model v0.5\n",
        "# Google Colab Notebook\n",
        "#\n",
        "# This version upgrades the multi-view architecture to use segmentation masks\n",
        "# for BOTH the side-view and top-view images, leveraging the new top-view\n",
        "# annotations for superior noise reduction and accuracy.\n",
        "#\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 1: SETUP AND IMPORTS\n",
        "# ==============================================================================\n",
        "!pip install tensorflow pandas scikit-learn pycocotools matplotlib\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, concatenate, Dropout\n",
        "from pycocotools.coco import COCO\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: MOUNT GOOGLE DRIVE AND DEFINE PATHS\n",
        "# ==============================================================================\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Drive mounted successfully.\")\n",
        "\n",
        "# --- IMPORTANT: UPDATE THESE PATHS ---\n",
        "BASE_DRIVE_PATH = '/content/drive/MyDrive/BovineMetrics_Dataset_v1/'\n",
        "SIDE_IMAGE_DIR = os.path.join(BASE_DRIVE_PATH, 'side_view_images')\n",
        "TOP_IMAGE_DIR = os.path.join(BASE_DRIVE_PATH, 'top_view_images')\n",
        "SIDE_ANNOTATIONS_PATH = os.path.join(BASE_DRIVE_PATH, 'side_view_annotations.json')\n",
        "TOP_ANNOTATIONS_PATH = os.path.join(BASE_DRIVE_PATH, 'top_view_annotations.json')\n",
        "METADATA_CSV_PATH = os.path.join(BASE_DRIVE_PATH, 'cattle_data.csv')\n",
        "# ------------------------------------\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: LOAD AND PREPROCESS THE DATA (UPGRADED FOR DUAL ANNOTATIONS)\n",
        "# ==============================================================================\n",
        "print(\"\\nLoading metadata from CSV...\")\n",
        "metadata_df = pd.read_csv(METADATA_CSV_PATH)\n",
        "metadata_df.columns = metadata_df.columns.str.strip().str.lower()\n",
        "print(f\"Loaded {len(metadata_df)} records from CSV.\")\n",
        "\n",
        "print(\"Loading COCO annotations for side-views...\")\n",
        "coco_side = COCO(SIDE_ANNOTATIONS_PATH)\n",
        "side_filename_to_id = {img['file_name']: img['id'] for img in coco_side.dataset['images']}\n",
        "print(f\"Loaded {len(coco_side.getImgIds())} side-view annotations.\")\n",
        "\n",
        "print(\"Loading COCO annotations for top-views...\")\n",
        "coco_top = COCO(TOP_ANNOTATIONS_PATH)\n",
        "top_filename_to_id = {img['file_name']: img['id'] for img in coco_top.dataset['images']}\n",
        "print(f\"Loaded {len(coco_top.getImgIds())} top-view annotations.\")\n",
        "\n",
        "def load_and_preprocess_multi_view_data(metadata_df, coco_side, coco_top, side_dir, top_dir):\n",
        "    side_images = []\n",
        "    top_images = []\n",
        "    tabular_data = []\n",
        "    weights = []\n",
        "    filenames_for_analysis = []\n",
        "\n",
        "    IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
        "    placeholder_image = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=\"uint8\")\n",
        "\n",
        "    for index, row in metadata_df.iterrows():\n",
        "        # --- Process Side View Image with Mask ---\n",
        "        side_filename = row.get('side_view_filename')\n",
        "        if pd.notna(side_filename) and isinstance(side_filename, str):\n",
        "            side_path = os.path.join(side_dir, side_filename)\n",
        "            img_id = side_filename_to_id.get(side_filename)\n",
        "            if os.path.exists(side_path) and img_id:\n",
        "                image = cv2.imread(side_path)\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
        "                ann_ids = coco_side.getAnnIds(imgIds=img_id)\n",
        "                anns = coco_side.loadAnns(ann_ids)\n",
        "                for ann in anns:\n",
        "                    for seg in ann['segmentation']:\n",
        "                        poly = np.array(seg).reshape((-1, 2)).astype(np.int32)\n",
        "                        cv2.fillPoly(mask, [poly], 255)\n",
        "\n",
        "                masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
        "                side_images.append(cv2.resize(masked_image, (IMG_WIDTH, IMG_HEIGHT)))\n",
        "            else:\n",
        "                side_images.append(placeholder_image)\n",
        "        else:\n",
        "            side_images.append(placeholder_image)\n",
        "\n",
        "        # --- Process Top View Image with Mask ---\n",
        "        top_filename = row.get('top_view_filename')\n",
        "        if pd.notna(top_filename) and isinstance(top_filename, str):\n",
        "            top_path = os.path.join(top_dir, top_filename)\n",
        "            img_id = top_filename_to_id.get(top_filename)\n",
        "            if os.path.exists(top_path) and img_id:\n",
        "                image = cv2.imread(top_path)\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
        "                ann_ids = coco_top.getAnnIds(imgIds=img_id)\n",
        "                anns = coco_top.loadAnns(ann_ids)\n",
        "                for ann in anns:\n",
        "                    for seg in ann['segmentation']:\n",
        "                        poly = np.array(seg).reshape((-1, 2)).astype(np.int32)\n",
        "                        cv2.fillPoly(mask, [poly], 255)\n",
        "\n",
        "                masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
        "                top_images.append(cv2.resize(masked_image, (IMG_WIDTH, IMG_HEIGHT)))\n",
        "            else:\n",
        "                top_images.append(placeholder_image)\n",
        "        else:\n",
        "            top_images.append(placeholder_image)\n",
        "\n",
        "        # --- Append Tabular Data and Weight ---\n",
        "        tabular_data.append(row[['sex', 'age', 'breed']])\n",
        "        weights.append(row['weight'])\n",
        "        filenames_for_analysis.append(side_filename if pd.notna(side_filename) else top_filename)\n",
        "\n",
        "    return (np.array(side_images, dtype=\"float32\"),\n",
        "            np.array(top_images, dtype=\"float32\"),\n",
        "            pd.DataFrame(tabular_data).reset_index(drop=True),\n",
        "            np.array(weights, dtype=\"float32\"),\n",
        "            filenames_for_analysis)\n",
        "\n",
        "side_images, top_images, tabular_data, weights, image_filenames = \\\n",
        "    load_and_preprocess_multi_view_data(metadata_df, coco_side, coco_top, SIDE_IMAGE_DIR, TOP_IMAGE_DIR)\n",
        "\n",
        "print(f\"\\nSuccessfully processed {len(weights)} records.\")\n",
        "\n",
        "# --- Normalize and Preprocess ---\n",
        "weight_mean = weights.mean()\n",
        "weight_std = weights.std()\n",
        "normalized_weights = (weights - weight_mean) / weight_std\n",
        "\n",
        "categorical_features = ['sex', 'breed']\n",
        "numerical_features = ['age']\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "processed_tabular = preprocessor.fit_transform(tabular_data)\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 4: DATA SPLITTING\n",
        "# ==============================================================================\n",
        "(train_side, val_side,\n",
        " train_top, val_top,\n",
        " train_tabular, val_tabular,\n",
        " train_weights, val_weights) = train_test_split(\n",
        "    side_images, top_images, processed_tabular, normalized_weights, test_size=0.2, random_state=42)\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 5: BUILD THE MULTI-VIEW MODEL (v0.5)\n",
        "# ==============================================================================\n",
        "print(\"\\nBuilding Masked Multi-View model with dual EfficientNetB0 inputs...\")\n",
        "\n",
        "def create_multi_view_model(image_shape, tabular_shape):\n",
        "    # --- Create a SINGLE, shared base model to avoid name collisions and download errors ---\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=image_shape)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # --- Side-View Branch (reuses the base model) ---\n",
        "    side_input = Input(shape=image_shape, name=\"side_view_input\")\n",
        "    x = base_model(side_input, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    side_features = Dense(64, activation='relu')(x)\n",
        "\n",
        "    # --- Top-View Branch (reuses the SAME base model) ---\n",
        "    top_input = Input(shape=image_shape, name=\"top_view_input\")\n",
        "    y = base_model(top_input, training=False)\n",
        "    y = GlobalAveragePooling2D()(y)\n",
        "    top_features = Dense(64, activation='relu')(y)\n",
        "\n",
        "    # --- Tabular Branch ---\n",
        "    tabular_input = Input(shape=(tabular_shape,), name=\"tabular_input\")\n",
        "    z = Dense(64, activation='relu')(tabular_input)\n",
        "    z = Dropout(0.3)(z)\n",
        "    z = Dense(32, activation='relu')(z)\n",
        "    tabular_features = z\n",
        "\n",
        "    # --- Combined Head ---\n",
        "    combined = concatenate([side_features, top_features, tabular_features])\n",
        "    c = Dense(128, activation='relu')(combined)\n",
        "    c = Dropout(0.5)(c)\n",
        "    c = Dense(64, activation='relu')(c)\n",
        "    output = Dense(1, name=\"output\")(c)\n",
        "\n",
        "    model = Model(inputs=[side_input, top_input, tabular_input], outputs=output)\n",
        "    return model\n",
        "\n",
        "image_shape = (224, 224, 3)\n",
        "tabular_shape = processed_tabular.shape[1]\n",
        "model = create_multi_view_model(image_shape, tabular_shape)\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "model.summary()\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 6: TRAIN THE MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\nStarting model training...\")\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# For simplicity in this version, we will not use the ImageDataGenerator.\n",
        "# As the dataset grows, we can re-introduce it with a more complex generator.\n",
        "history = model.fit(\n",
        "    {\"side_view_input\": train_side, \"top_view_input\": train_top, \"tabular_input\": train_tabular},\n",
        "    train_weights,\n",
        "    validation_data=(\n",
        "        {\"side_view_input\": val_side, \"top_view_input\": val_top, \"tabular_input\": val_tabular},\n",
        "        val_weights\n",
        "    ),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True), lr_scheduler]\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 7: EVALUATE THE MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\nEvaluating final model on validation data...\")\n",
        "\n",
        "val_input = {\"side_view_input\": val_side, \"top_view_input\": val_top, \"tabular_input\": val_tabular}\n",
        "predictions_norm = model.predict(val_input).flatten()\n",
        "predictions_kg = (predictions_norm * weight_std) + weight_mean\n",
        "val_weights_kg = (val_weights * weight_std) + weight_mean\n",
        "\n",
        "final_mae = np.mean(np.abs(val_weights_kg - predictions_kg))\n",
        "final_mape = np.mean(np.abs((val_weights_kg - predictions_kg) / val_weights_kg)) * 100\n",
        "\n",
        "print(\"\\n--- PILOT MODEL v0.5 RESULTS ---\")\n",
        "print(f\"Final Validation Mean Absolute Error (MAE): {final_mae:.2f} kg\")\n",
        "print(f\"Final Validation Mean Absolute Percentage Error (MAPE): {final_mape:.2f}%\")\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 8: ERROR ANALYSIS & VISUALIZATION\n",
        "# ==============================================================================\n",
        "print(\"\\nStarting error analysis...\")\n",
        "\n",
        "full_dataset_input = {\n",
        "    \"side_view_input\": side_images,\n",
        "    \"top_view_input\": top_images,\n",
        "    \"tabular_input\": processed_tabular\n",
        "}\n",
        "full_dataset_predictions_norm = model.predict(full_dataset_input).flatten()\n",
        "full_dataset_predictions_kg = (full_dataset_predictions_norm * weight_std) + weight_mean\n",
        "\n",
        "error_df = tabular_data.copy()\n",
        "error_df['image_filename'] = image_filenames\n",
        "error_df['actual_weight_kg'] = weights\n",
        "error_df['predicted_weight_kg'] = full_dataset_predictions_kg\n",
        "error_df['error_kg'] = error_df['predicted_weight_kg'] - error_df['actual_weight_kg']\n",
        "error_df['abs_error_kg'] = np.abs(error_df['error_kg'])\n",
        "\n",
        "error_df_sorted = error_df.sort_values(by='abs_error_kg', ascending=False)\n",
        "\n",
        "print(\"\\n--- TOP 20 WORST PREDICTIONS ---\")\n",
        "print(error_df_sorted[['image_filename', 'actual_weight_kg', 'predicted_weight_kg', 'error_kg']].head(20))\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(error_df['actual_weight_kg'], error_df['predicted_weight_kg'], alpha=0.6, edgecolors='k')\n",
        "lims = [ np.min([plt.xlim(), plt.ylim()]), np.max([plt.xlim(), plt.ylim()]) ]\n",
        "plt.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label=\"Perfect Prediction\")\n",
        "plt.xlabel(\"Actual Weight (kg)\")\n",
        "plt.ylabel(\"Predicted Weight (kg)\")\n",
        "plt.title(\"Model Performance: Actual vs. Predicted Weight\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (2.0.10)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "TensorFlow Version: 2.19.0\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted successfully.\n",
            "\n",
            "Loading metadata from CSV...\n",
            "Loaded 696 records from CSV.\n",
            "Loading COCO annotations for side-views...\n",
            "loading annotations into memory...\n",
            "Done (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "Loaded 608 side-view annotations.\n",
            "Loading COCO annotations for top-views...\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Loaded 106 top-view annotations.\n",
            "\n",
            "Successfully processed 696 records.\n",
            "\n",
            "Building Masked Multi-View model with dual EfficientNetB0 inputs...\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnet_side_notop.h5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "URL fetch failure on https://storage.googleapis.com/keras-applications/efficientnet_side_notop.h5: 403 -- Forbidden",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/file_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDLProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    631\u001b[0m                 'http', request, response, code, msg, hdrs)\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3368770143.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0mimage_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0mtabular_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_tabular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_multi_view_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3368770143.py\u001b[0m in \u001b[0;36mcreate_multi_view_model\u001b[0;34m(image_shape, tabular_shape)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mside_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"side_view_input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# *** FIX: Add unique name to the model ***\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mbase_model_side\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientNetB0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"efficientnet_side\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mbase_model_side\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model_side\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mside_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNetB0\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"efficientnetb0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m ):\n\u001b[0;32m--> 571\u001b[0;31m     return EfficientNet(\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNet\u001b[0;34m(width_coefficient, depth_coefficient, default_size, dropout_rate, drop_connect_rate, depth_divisor, activation, blocks_args, name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, weights_name)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mfile_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWEIGHTS_HASHES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweights_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         weights_path = file_utils.get_file(\n\u001b[0m\u001b[1;32m    429\u001b[0m             \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mBASE_WEIGHTS_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/file_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDLProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: URL fetch failure on https://storage.googleapis.com/keras-applications/efficientnet_side_notop.h5: 403 -- Forbidden"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3L8H-SR3kpA5",
        "outputId": "f8071949-0b8f-4b14-e268-aa1aaf6ebab5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}