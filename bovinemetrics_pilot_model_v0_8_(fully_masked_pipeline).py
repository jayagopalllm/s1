# -*- coding: utf-8 -*-
"""BovineMetrics - Pilot Model v0.8 (Fully Masked Pipeline)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wvlRxkPBf8bP70_uI3PUkaS34H3LE-T9
"""

#
# Project BovineMetrics - Pilot Model v0.8
# Google Colab Notebook
#
# This version introduces a fully masked `tf.data` pipeline using `tf.py_function`.
# This is a major architectural upgrade that allows us to apply our complex
# COCO segmentation masks inside the high-performance data pipeline. It correctly
# handles missing images and forces the model to learn from pixel data.
#

# ==============================================================================
# STEP 1: SETUP AND IMPORTS
# ==============================================================================
!pip install tensorflow pandas scikit-learn pycocotools matplotlib

import os
import numpy as np
import pandas as pd
import cv2
from google.colab import drive
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, concatenate, Dropout, RandomFlip, RandomRotation, RandomZoom
from pycocotools.coco import COCO
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt

print("TensorFlow Version:", tf.__version__)

# ==============================================================================
# STEP 2: MOUNT GOOGLE DRIVE AND DEFINE PATHS
# ==============================================================================
print("Mounting Google Drive...")
drive.mount('/content/drive')
print("Drive mounted successfully.")

# --- IMPORTANT: UPDATE THESE PATHS ---
BASE_DRIVE_PATH = '/content/drive/MyDrive/dataset/'
SIDE_IMAGE_DIR = os.path.join(BASE_DRIVE_PATH, 'side_view_images')
TOP_IMAGE_DIR = os.path.join(BASE_DRIVE_PATH, 'top_view_images')
SIDE_ANNOTATIONS_PATH = os.path.join(BASE_DRIVE_PATH, 'side_view_annotations.json')
TOP_ANNOTATIONS_PATH = os.path.join(BASE_DRIVE_PATH, 'top_view_annotations.json')
METADATA_CSV_PATH = os.path.join(BASE_DRIVE_PATH, 'cattle_data.csv')
# ------------------------------------

# --- Create a placeholder image for missing views ---
IMG_HEIGHT, IMG_WIDTH = 224, 224
PLACEHOLDER_PATH = os.path.join(BASE_DRIVE_PATH, "placeholder.jpg")
if not os.path.exists(PLACEHOLDER_PATH):
    placeholder_image = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype="uint8")
    cv2.imwrite(PLACEHOLDER_PATH, placeholder_image)
    print("Created placeholder image for missing views.")

# ==============================================================================
# STEP 3: LOAD AND PREPROCESS THE DATA (REFACTORED for tf.data)
# ==============================================================================
print("\nLoading metadata from CSV...")
metadata_df = pd.read_csv(METADATA_CSV_PATH)
metadata_df.columns = metadata_df.columns.str.strip().str.lower()
print(f"Loaded {len(metadata_df)} records from CSV.")

print("Loading COCO annotations...")
coco_side = COCO(SIDE_ANNOTATIONS_PATH)
coco_top = COCO(TOP_ANNOTATIONS_PATH)
side_filename_to_id = {img['file_name']: img['id'] for img in coco_side.dataset['images']}
top_filename_to_id = {img['file_name']: img['id'] for img in coco_top.dataset['images']}

# --- Create a unified DataFrame with all paths and data ---
data_records = []
for _, row in metadata_df.iterrows():
    record = row.to_dict()
    # Replace missing filenames with the placeholder path
    side_fname = str(row.get('side_view_filename', ''))
    top_fname = str(row.get('top_view_filename', ''))

    record['side_view_path'] = os.path.join(SIDE_IMAGE_DIR, side_fname) if pd.notna(row.get('side_view_filename')) else PLACEHOLDER_PATH
    record['top_view_path'] = os.path.join(TOP_IMAGE_DIR, top_fname) if pd.notna(row.get('top_view_filename')) else PLACEHOLDER_PATH

    data_records.append(record)

full_df = pd.DataFrame(data_records)

# --- Preprocess Tabular Data ---
categorical_features, numerical_features = ['sex', 'breed'], ['age']
preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), numerical_features),
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])
processed_tabular = preprocessor.fit_transform(full_df[categorical_features + numerical_features])

# --- Normalize Weights ---
weights = full_df['weight'].values.astype(np.float32)
weight_mean, weight_std = weights.mean(), weights.std()
normalized_weights = (weights - weight_mean) / weight_std

# --- Split data before creating TensorFlow Datasets ---
train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42)

# ==============================================================================
# STEP 4: CREATE THE `tf.data` PIPELINE WITH MASKING (v0.8)
# ==============================================================================
print("\nBuilding `tf.data` input pipeline with segmentation masking...")
BATCH_SIZE = 16
AUTOTUNE = tf.data.AUTOTUNE

def apply_mask(image_path_str, coco_instance, filename_to_id_map):
    """Helper function to read an image and apply its COCO mask."""
    # Read image using OpenCV
    image = cv2.imread(image_path_str)

    # Check if image was successfully loaded
    if image is None:
        print(f"Warning: Could not load image from {image_path_str}. Returning black image.")
        return np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype="uint8")

    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Get filename to find annotations
    filename = os.path.basename(image_path_str)
    img_id = filename_to_id_map.get(filename)

    # Create mask and apply it
    if img_id:
        mask = np.zeros(image.shape[:2], dtype="uint8")
        ann_ids = coco_instance.getAnnIds(imgIds=img_id)
        anns = coco_instance.loadAnns(ann_ids)
        for ann in anns:
            for seg in ann['segmentation']:
                poly = np.array(seg).reshape((-1, 2)).astype(np.int32)
                cv2.fillPoly(mask, [poly], 255)
        masked_image = cv2.bitwise_and(image, image, mask=mask)
        return cv2.resize(masked_image, (IMG_WIDTH, IMG_HEIGHT))
    else:
        # This handles the placeholder or images without annotations
        return cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))

def process_path(side_path, top_path):
    # Decode path tensors from TensorFlow to Python strings
    side_path_str = side_path.numpy().decode('utf-8')
    top_path_str = top_path.numpy().decode('utf-8')

    side_img = apply_mask(side_path_str, coco_side, side_filename_to_id)
    top_img = apply_mask(top_path_str, coco_top, top_filename_to_id)

    return side_img.astype(np.float32), top_img.astype(np.float32)

def configure_dataset(df, tabular_data, weights):
    dataset = tf.data.Dataset.from_tensor_slices((
        (df['side_view_path'].values, df['top_view_path'].values),
        tabular_data[df.index],
        weights[df.index]
    ))
    dataset = dataset.shuffle(buffer_size=len(df))

    # Use tf.py_function to wrap our Python masking logic
    def py_func_wrapper(paths, tab, w):
        side_img, top_img = tf.py_function(
            process_path,
            [paths[0], paths[1]],
            [tf.float32, tf.float32]
        )
        side_img.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])
        top_img.set_shape([IMG_HEIGHT, IMG_WIDTH, 3])
        return {"side_view_input": side_img, "top_view_input": top_img, "tabular_input": tab}, w

    dataset = dataset.map(py_func_wrapper, num_parallel_calls=AUTOTUNE)
    dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)
    return dataset

train_dataset = configure_dataset(train_df, processed_tabular, normalized_weights)
val_dataset = configure_dataset(val_df, processed_tabular, normalized_weights)

# ==============================================================================
# STEP 5: BUILD THE MULTI-VIEW MODEL WITH AUGMENTATION LAYERS (v0.8)
# ==============================================================================
print("\nBuilding Masked Multi-View model with integrated augmentation...")

data_augmentation = tf.keras.Sequential([
    RandomFlip("horizontal"),
    RandomRotation(0.1),
    RandomZoom(0.1),
], name="data_augmentation")

def create_multi_view_model(image_shape, tabular_shape):
    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=image_shape)
    base_model.trainable = False

    side_input = Input(shape=image_shape, name="side_view_input")
    augmented_side = data_augmentation(side_input)
    x = base_model(augmented_side, training=False)
    x = GlobalAveragePooling2D()(x)
    side_features = Dense(64, activation='relu')(x)

    top_input = Input(shape=image_shape, name="top_view_input")
    augmented_top = data_augmentation(top_input)
    y = base_model(augmented_top, training=False)
    y = GlobalAveragePooling2D()(y)
    top_features = Dense(64, activation='relu')(y)

    tabular_input = Input(shape=(tabular_shape,), name="tabular_input")
    z = Dense(64, activation='relu')(tabular_input)
    z = Dropout(0.3)(z)
    z = Dense(32, activation='relu')(z)
    tabular_features = z

    combined = concatenate([side_features, top_features, tabular_features])
    c = Dense(128, activation='relu')(combined)
    c = Dropout(0.5)(c)
    c = Dense(64, activation='relu')(c)
    output = Dense(1, name="output")(c)

    model = Model(inputs=[side_input, top_input, tabular_input], outputs=output)
    return model

image_shape = (IMG_HEIGHT, IMG_WIDTH, 3)
tabular_shape = processed_tabular.shape[1]
model = create_multi_view_model(image_shape, tabular_shape)

# ==============================================================================
# STEP 6: INITIAL TRAINING (HEAD ONLY)
# ==============================================================================
print("\n--- Starting Initial Training (Frozen Base Model) ---")
INITIAL_EPOCHS = 100

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='mean_squared_error', metrics=['mean_absolute_error'])

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=INITIAL_EPOCHS,
    callbacks=[EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)])

# ==============================================================================
# STEP 7: FINE-TUNING STAGE
# ==============================================================================
print("\n--- Starting Fine-Tuning Stage (Unfreezing Top Layers) ---")

base_model = model.layers[4]
base_model.trainable = True
fine_tune_at = len(base_model.layers) // 10 * 6
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss='mean_squared_error', metrics=['mean_absolute_error'])
model.summary()

FINE_TUNE_EPOCHS = 50
total_epochs = history.epoch[-1] + FINE_TUNE_EPOCHS

history_fine = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=total_epochs,
    initial_epoch=history.epoch[-1],
    callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)])

# ==============================================================================
# STEP 8: EVALUATE THE FINAL MODEL
# ==============================================================================
print("\nEvaluating final model on validation data...")
# Create a non-shuffled validation dataset for consistent evaluation
val_dataset_no_shuffle = configure_dataset(val_df, processed_tabular, normalized_weights)
val_dataset_no_shuffle = val_dataset_no_shuffle.map(lambda x, y: (x, y))

predictions_norm = model.predict(val_dataset_no_shuffle).flatten()
val_weights_kg = val_df['weight'].values
predictions_kg = (predictions_norm * weight_std) + weight_mean
predictions_kg = predictions_kg[:len(val_weights_kg)]

final_mae = np.mean(np.abs(val_weights_kg - predictions_kg))
final_mape = np.mean(np.abs((val_weights_kg - predictions_kg) / val_weights_kg)) * 100

print("\n--- PILOT MODEL v0.8 RESULTS ---")
print(f"Final Validation Mean Absolute Error (MAE): {final_mae:.2f} kg")
print(f"Final Validation Mean Absolute Percentage Error (MAPE): {final_mape:.2f}%")
print("---------------------------------")

# ==============================================================================
# STEP 9: ERROR ANALYSIS & VISUALIZATION
# ==============================================================================
print("\nStarting error analysis...")
# Create a full, non-shuffled dataset for prediction
full_dataset_no_shuffle = configure_dataset(full_df, processed_tabular, normalized_weights)
full_dataset_no_shuffle = full_dataset_no_shuffle.map(lambda x, y: (x, y))

full_dataset_predictions_norm = model.predict(full_dataset_no_shuffle).flatten()
full_dataset_predictions_kg = (full_dataset_predictions_norm * weight_std) + weight_mean
full_dataset_predictions_kg = full_dataset_predictions_kg[:len(full_df)]

error_df = full_df.copy()
error_df['predicted_weight_kg'] = full_dataset_predictions_kg
error_df['error_kg'] = error_df['predicted_weight_kg'] - error_df['weight'] # Corrected column name to 'weight'
error_df['abs_error_kg'] = np.abs(error_df['error_kg'])
error_df_sorted = error_df.sort_values(by='abs_error_kg', ascending=False)

print("\n--- TOP 20 WORST PREDICTIONS ---")
print(error_df_sorted[['side_view_filename', 'weight', 'predicted_weight_kg', 'error_kg']].head(20)) # Corrected column name to 'weight'
print("---------------------------------")

plt.figure(figsize=(10, 10))
plt.scatter(error_df['weight'], error_df['predicted_weight_kg'], alpha=0.6, edgecolors='k') # Corrected column name to 'weight'
lims = [ np.min([error_df['weight'], error_df['predicted_weight_kg']]), # Corrected column name to 'weight'
         np.max([error_df['weight'], error_df['predicted_weight_kg']]) ] # Corrected column name to 'weight'
plt.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label="Perfect Prediction")
plt.xlabel("Actual Weight (kg)")
plt.ylabel("Predicted Weight (kg)")
plt.title("Model Performance: Actual vs. Predicted Weight")
plt.legend()
plt.grid(True)
plt.show()